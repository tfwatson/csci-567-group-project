{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjXoiJUbT0eb",
        "outputId": "71688455-bdbb-4c6b-9f7b-d19cc5c49764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.15.0)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRWPB1C2TJ4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10e0f18-27ee-4954-f120-7885ccf60359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6j4oOClThBZ",
        "outputId": "15ff96a2-82f3-474c-bf52-6bedf4becad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUaVOMZjTJ4P"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNTjn98MTJ4Q"
      },
      "outputs": [],
      "source": [
        "vocab,embeddings = [],[]\n",
        "with open('drive/MyDrive/csci_567_project/glove.6B.100d.txt','rt', encoding='utf8') as fi:\n",
        "    full_content = fi.read().strip().split('\\n')\n",
        "for i in range(len(full_content)):\n",
        "    i_word = full_content[i].split(' ')[0]\n",
        "    i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "    vocab.append(i_word)\n",
        "    embeddings.append(i_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kazs8tuFTJ4Q"
      },
      "outputs": [],
      "source": [
        "vocab_npa = np.array(vocab)\n",
        "embs_npa = np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOqbN6uiTJ4Q"
      },
      "outputs": [],
      "source": [
        "vocab_npa = np.insert(vocab_npa, 0, '[PAD]')\n",
        "vocab_npa = np.insert(vocab_npa, 1, '[UNK]')\n",
        "\n",
        "pad_emb_npa = np.zeros((1,embs_npa.shape[1]))   #embedding for '<pad>' token.\n",
        "unk_emb_npa = np.mean(embs_npa,axis=0,keepdims=True)    #embedding for '<unk>' token.\n",
        "\n",
        "#insert embeddings for pad and unk tokens at top of embs_npa.\n",
        "embs_npa = np.vstack((pad_emb_npa,unk_emb_npa,embs_npa))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTy4LrpjTJ4Q"
      },
      "outputs": [],
      "source": [
        "my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float(), freeze=True, padding_idx=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p4wN3XYTJ4Q"
      },
      "outputs": [],
      "source": [
        "word2idx = {\n",
        "    word: idx\n",
        "    for idx, word in enumerate(vocab_npa)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(word2idx.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_jhzrWIf3fT",
        "outputId": "756b8cc1-b9b6-4521-e4dc-684be4d97c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVlA4ii3TJ4Q"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('drive/MyDrive/csci_567_project/train_text.csv')\n",
        "\n",
        "from tokenizers import pre_tokenizers\n",
        "from tokenizers.pre_tokenizers import Digits, Whitespace\n",
        "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
        "\n",
        "def sentiment_to_tensor(s: str):\n",
        "    if s == 'positive':\n",
        "        return 0\n",
        "    elif s == 'neutral':\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "def utterance_split(u: str):\n",
        "    obj = pre_tokenizer.pre_tokenize_str(u)\n",
        "    return [x[0].lower() for x in obj]\n",
        "\n",
        "\n",
        "df = pd.concat([df['Utterance'].map(lambda x: utterance_split(x)), df['Sentiment'].map(lambda x: sentiment_to_tensor(x))],\n",
        "               keys=['data', 'labels'])\n",
        "\n",
        "df['data'] = df['data'].map(lambda x: [word2idx.get(word, 1) for word in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ouy-EC9CTJ4R"
      },
      "outputs": [],
      "source": [
        "# Tunable hyperparameter\n",
        "batch_size = 16\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        # We have to transpose after pad_sequence since pad_sequence performs a transpose\n",
        "        self.data = torch.transpose(nn.utils.rnn.pad_sequence(list(map(lambda x: torch.LongTensor(x), df['data']))), 0, 1)\n",
        "\n",
        "        self.labels = torch.LongTensor(list(labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.size()[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "text_dataset = TextDataset(df['data'], df['labels'])\n",
        "text_dataloader = DataLoader(text_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9MB6gbbTJ4R"
      },
      "outputs": [],
      "source": [
        "# Model architecture parameters\n",
        "vocab_size = len(word2idx.keys())\n",
        "embedding_size = 100\n",
        "padding_index = 0\n",
        "num_classes = 3\n",
        "\n",
        "# Tunable hyperparameters\n",
        "hidden_layers = 1\n",
        "hidden_layer_size = 256\n",
        "dropout_probability = 0.33\n",
        "linear_output_size = 128\n",
        "num_directions = 2\n",
        "elu_alpha = 1\n",
        "learning_rate = 0.001\n",
        "scheduler_gamma = 0.9\n",
        "\n",
        "class GLoVeLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GLoVeLSTM, self).__init__()\n",
        "\n",
        "        self.embedding = my_embedding_layer\n",
        "        self.lstm = nn.LSTM(input_size=embedding_size, num_layers=hidden_layers, hidden_size=hidden_layer_size,\n",
        "                            bidirectional=(True if num_directions == 2 else False), batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=dropout_probability)\n",
        "        self.linear = nn.Linear(in_features=hidden_layer_size * num_directions, out_features=linear_output_size)\n",
        "        self.activation = nn.ELU(alpha=elu_alpha)\n",
        "        self.classifier = nn.Linear(in_features=linear_output_size, out_features=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.classifier(x)\n",
        "        return x[:, -1, :]\n",
        "\n",
        "glove_lstm = GLoVeLSTM().to(device)\n",
        "\n",
        "# Tunable Optimizer, Scheduler, and Loss Function\n",
        "optimizer = torch.optim.AdamW(params=glove_lstm.parameters(), lr=learning_rate)\n",
        "# scheduler = torch.optim.lr_scheduler.ExponentialLR(gamma=scheduler_gamma, optimizer=optimizer)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyrZok17TJ4R"
      },
      "outputs": [],
      "source": [
        "# Tunable hyperparameters\n",
        "num_epochs = 25\n",
        "\n",
        "valid_loss_min = np.Inf\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "\n",
        "    glove_lstm.train()\n",
        "    for data, target in text_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        output = glove_lstm(data)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/csci_567_project/dev_text.csv')\n",
        "\n",
        "from tokenizers import pre_tokenizers\n",
        "from tokenizers.pre_tokenizers import Digits, Whitespace\n",
        "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
        "\n",
        "\n",
        "df = pd.concat([df['Utterance'].map(lambda x: utterance_split(x)), df['Sentiment'].map(lambda x: sentiment_to_tensor(x))],\n",
        "               keys=['data', 'labels'])\n",
        "\n",
        "df['data'] = df['data'].map(lambda x: [word2idx.get(word, 1) for word in x])"
      ],
      "metadata": {
        "id": "U6hwWE-lU0qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset = TextDataset(df['data'], df['labels'])\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Xiu87EGWVEZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss= 0.0\n",
        "correct, total = 0, 0\n",
        "\n",
        "for data, label in dev_dataloader:\n",
        "    glove_lstm.eval()\n",
        "    data = data.to(device)\n",
        "    label = label.to(device)\n",
        "\n",
        "    output = glove_lstm(data)\n",
        "    loss = criterion(output, label)\n",
        "    for o,l in zip(torch.argmax(output,axis = 1),label):\n",
        "        if o == l:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    loss = criterion(output,label)\n",
        "    test_loss += loss.item() * data.size(0)\n",
        "\n",
        "print('Test Loss: ' + str(test_loss / len(dev_dataloader.dataset)))\n",
        "print('Correct Guesses: ' + str(correct) + '/' + str(total) + ' -> Accuracy: ' + str(correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPSP0I5MVOVK",
        "outputId": "2c61236c-ff90-403e-a6a5-3cac879be473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0741448338984154\n",
            "Correct Guesses: 470/1109 -> Accuracy: 0.4238052299368801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(glove_lstm, 'drive/MyDrive/csci_567_project/glove_lstm.pt')"
      ],
      "metadata": {
        "id": "KKtgt79XaUBK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}